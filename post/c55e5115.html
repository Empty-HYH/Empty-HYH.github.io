<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark RDD 常用算子 | Eurkon</title><meta name="keywords" content="大数据,Spark"><meta name="author" content="Eurkon,eurkon@foxmail.com"><meta name="copyright" content="Eurkon"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Spark 的算子的分类 从大方向来说，Spark 算子大致可以分为以下两类:  Transformation 变换&#x2F;转换算子：这种变换并不触发提交作业，完成作业中间过程处理。 Transformation 操作是延迟计算的，也就是说从一个 RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。  Action 行动算子：这类算子会触发 Sp">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark RDD 常用算子">
<meta property="og:url" content="https://blog.eurkon.com/post/c55e5115.html">
<meta property="og:site_name" content="Eurkon">
<meta property="og:description" content="Spark 的算子的分类 从大方向来说，Spark 算子大致可以分为以下两类:  Transformation 变换&#x2F;转换算子：这种变换并不触发提交作业，完成作业中间过程处理。 Transformation 操作是延迟计算的，也就是说从一个 RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。  Action 行动算子：这类算子会触发 Sp">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://blog.eurkon.com/images/cover/spark_rdd.jpg">
<meta property="article:published_time" content="2021-05-07T01:00:00.000Z">
<meta property="article:modified_time" content="2021-05-20T08:56:14.194Z">
<meta property="article:author" content="Eurkon">
<meta property="article:tag" content="大数据">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://blog.eurkon.com/images/cover/spark_rdd.jpg"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/favicon.png"><link rel="canonical" href="https://blog.eurkon.com/post/c55e5115"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="rClKrjtCYPsDaynvUNmfe2YGPxb7ehnuwEF9aMdG7no"/><meta name="msvalidate.01" content="F299EA4E7AD0E2B28FDF4E0EA94879BA"/><meta name="baidu-site-verification" content="code-GmAI2TORJk"/><meta name="360-site-verification" content="ac410c1012e6f0acc65b5c0805c91f01"/><meta name="yandex-verification" content="ac410c1012e6f0acc65b5c0805c91f01"/><meta name="bytedance-verification-code" content="D4gz9gUv9Wh0pS4d20uQ"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?ca4902422e728abe0603d52ce2e7757a";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/4.1.1/animate.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":250},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Eurkon","link":"链接: ","source":"来源: Eurkon","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"top-right"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark RDD 常用算子',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2021-05-20 16:56:14'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const fontSizeVal = saveToLocal.get('global-font-size')
    if (fontSizeVal !== undefined) {
      document.documentElement.style.setProperty('--global-font-size', fontSizeVal + 'px')
    }
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/pace-js@latest/pace.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/dist/echarts.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/npm/echarts@4.7.0/map/js/china.js"></script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Swiper/4.1.6/css/swiper.min.css"><style>@import url("https://fonts.googleapis.com/css?family=Fira+Sans:400,500,600,700,800");*{box-sizing:border-box}.blog-slider{width:100%;position:relative;border-radius:12px 8px 8px 12px;margin:auto;background:var(--global-bg) padding:10px;transition:all .3s}@media screen and (max-width:768px){.blog-slider{min-height:350px;height:auto;margin-top:110px;margin-bottom:10px}}@media screen and (max-height:500px) and (min-width:992px){.blog-slider{height:350px}}.blog-slider__item{display:flex;align-items:center}@media screen and (max-width:768px){.blog-slider__item{flex-direction:column}}.blog-slider__item.swiper-slide-active .blog-slider__img img{opacity:1;transition-delay:.3s}.blog-slider__item.swiper-slide-active .blog-slider__content>*{opacity:1;transform:none}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(1){transition-delay:.3s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(2){transition-delay:.4s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(3){transition-delay:.5s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(4){transition-delay:.6s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(5){transition-delay:.7s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(6){transition-delay:.8s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(7){transition-delay:.9s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(8){transition-delay:1s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(9){transition-delay:1.1s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(10){transition-delay:1.2s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(11){transition-delay:1.3s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(12){transition-delay:1.4s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(13){transition-delay:1.5s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(14){transition-delay:1.6s}.blog-slider__item.swiper-slide-active .blog-slider__content>*:nth-child(15){transition-delay:1.7s}.blog-slider__img{width:200px;flex-shrink:0;height:200px;padding:10px;border-radius:5px;transform:translateX(0px);overflow:hidden}.blog-slider__img:after{content:'';position:absolute;top:0;left:0;width:100%;height:100%;border-radius:5px;opacity:.8}.blog-slider__img img{width:100%;height:100%;object-fit:cover;display:block;opacity:0;border-radius:5px;transition:all .3s}@media screen and (max-width:768px){.blog-slider__img{transform:translateY(-50%);width:90%}}@media screen and (max-width:576px){.blog-slider__img{width:95%}}@media screen and (max-height:500px) and (min-width:992px){.blog-slider__img{height:270px}}.blog-slider__content{padding-right:50px;padding-left:50px}@media screen and (max-width:768px){.blog-slider__content{margin-top:-80px;text-align:center;padding:0 30px}}@media screen and (max-width:576px){.blog-slider__content{padding-left:10px;padding-right:10px}}.blog-slider__content>*{opacity:0;transform:translateY(25px);transition:all .4s}.blog-slider__code{color:var(--font-color);margin-bottom:0;display:block;font-weight:500}.blog-slider__title{font-size:18px;font-weight:700;color:var(--font-color);margin-bottom:15px;-webkit-line-clamp:1;display:-webkit-box;overflow:hidden;-webkit-box-orient:vertical}.blog-slider__text{color:var(--font-color);-webkit-line-clamp:1;display:-webkit-box;overflow:hidden;-webkit-box-orient:vertical;margin-bottom:15px;line-height:1.5em}.blog-slider__button{display:inline-flex;background-color:var(--btn-bg);padding:4px 14px;border-radius:8px;color:var(--btn-color);text-decoration:none;font-weight:500;justify-content:center;text-align:center;letter-spacing:1px}.blog-slider__button:hover{background-color:var(--btn-hover-color);color:var(--btn-color)}@media screen and (max-width:576px){.blog-slider__button{width:100%;width:100%}}.blog-slider .swiper-container-horizontal>.swiper-pagination-bullets,.blog-slider .swiper-pagination-custom,.blog-slider .swiper-pagination-fraction{bottom:10px;left:0;width:100%}.blog-slider__pagination{position:absolute;z-index:21;right:20px;width:11px!important;text-align:center;left:auto!important;top:50%;bottom:auto!important;transform:translateY(-50%)}@media screen and (max-width:768px){.blog-slider__pagination{transform:translateX(-50%);left:50%!important;top:320px;width:100%!important;display:flex;justify-content:center;align-items:center}}.blog-slider__pagination.swiper-pagination-bullets .swiper-pagination-bullet{margin:8px 0 !important}@media screen and (max-width:768px){.blog-slider__pagination.swiper-pagination-bullets .swiper-pagination-bullet{margin:0 5px}}.blog-slider__pagination .swiper-pagination-bullet{width:11px;height:11px;display:block;border-radius:10px;background:#858585;opacity:.2;transition:all .3s}.blog-slider__pagination .swiper-pagination-bullet-active{opacity:1;background:var(--btn-bg);height:30px} @media screen and (max-width:768px){.blog-slider__pagination .swiper-pagination-bullet-active{height:11px;width:30px}}.blog-slider__button{display:none}@media screen and (max-width:768px){.blog-slider__button{display:inline-flex}.blog-slider__text{margin-bottom:40px}}</style><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/avatar.jpg" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">47</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/random/"><i class="fa-fw fas fa-random"></i><span> 围观</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-link"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-address-card"></i><span> 友链</span></a></li><li><a class="site-page child" href="/stars/"><i class="fa-fw fas fa-star"></i><span> 收藏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-bug"></i><span> 实验室</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/fcircle/"><i class="fa-fw fas fa-spinner"></i><span> 朋友圈</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-line"></i><span> 博客统计</span></a></li><li><a class="site-page child" href="/charts/"><i class="fa-fw fas fa-chart-bar"></i><span> 文章统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-info-circle"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background:var(--global-bg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Eurkon</a></span><span id="page_name"><a id="page-name-text"></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-book-open"></i><span> 文章</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></li><li><a class="site-page child" href="/random/"><i class="fa-fw fas fa-random"></i><span> 围观</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-link"></i><span> 链接</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/link/"><i class="fa-fw fas fa-address-card"></i><span> 友链</span></a></li><li><a class="site-page child" href="/stars/"><i class="fa-fw fas fa-star"></i><span> 收藏</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);" rel="external nofollow noreferrer"><i class="fa-fw fas fa-bug"></i><span> 实验室</span><i class="fas fa-chevron-down expand hide"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/fcircle/"><i class="fa-fw fas fa-spinner"></i><span> 朋友圈</span></a></li><li><a class="site-page child" href="/census/"><i class="fa-fw fas fa-chart-line"></i><span> 博客统计</span></a></li><li><a class="site-page child" href="/charts/"><i class="fa-fw fas fa-chart-bar"></i><span> 文章统计</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-info-circle"></i><span> 关于</span></a></div></div></div><div id="hotkey"><div id="search-button"><a class="site-page social-icon search" title="搜索"><i class="fas fa-search fa-fw"></i></a></div><div id="mode-button"><a class="site-page" title="浅色和深色模式转换"><i class="fas fa-adjust fa-fw"></i></a></div><div id="comment-button"><a class="site-page" href="#post-comment" title="直达评论"><i class="fas fa-comments fa-fw"></i></a></div><div id="top-button"><a class="site-page" title="回到顶部"><i class="fas fa-rocket fa-fw"></i></a></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-cover"><img class="cover" id="post-cover-img" src="/images/cover/spark_rdd.jpg" alt="cover"></div><div id="post-info"><h1 class="post-title">Spark RDD 常用算子</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-05-07T01:00:00.000Z" title="发表于 2021-05-07 09:00:00">2021-05-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-05-20T08:56:14.194Z" title="更新于 2021-05-20 16:56:14">2021-05-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%95%99%E7%A8%8B%E7%AC%94%E8%AE%B0/">教程笔记</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">10,070</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>48分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark RDD 常用算子"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/post/c55e5115.html#post-comment"><span id="twikoo-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Spark-的算子的分类"><a href="#Spark-的算子的分类" class="headerlink" title="Spark 的算子的分类"></a>Spark 的算子的分类</h1><ul>
<li><p>从大方向来说，Spark 算子大致可以分为以下两类:</p>
<ul>
<li><p><strong>Transformation 变换/转换算子</strong>：这种变换并不触发提交作业，完成作业中间过程处理。</p>
<p>Transformation 操作是延迟计算的，也就是说从一个 RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</p>
</li>
<li><p><strong>Action 行动算子</strong>：这类算子会触发 SparkContext 提交 Job 作业。</p>
<p>Action 算子会触发 Spark 提交作业（Job），并将数据输出 Spark 系统。</p>
</li>
</ul>
</li>
<li><p>从小方向来说，Spark 算子大致可以分为以下三类:</p>
<ul>
<li><a href="#Value-数据类型的-Transformation-算子">Value 数据类型的 Transformation 算子</a>，这种变换并不触发提交作业，针对处理的数据项是 Value 型的数据。</li>
<li><a href="#Key-Value-数据类型的-Transformation-算子">Key-Value 数据类型的 Transformation 算子</a>，这种变换并不触发提交作业，针对处理的数据项是 Key-Value 型的数据对。</li>
<li><a href="#Action-算子">Action 算子</a>，这类算子会触发 SparkContext 提交 Job 作业。</li>
</ul>
</li>
</ul>
<h1 id="Value-数据类型的-Transformation-算子"><a href="#Value-数据类型的-Transformation-算子" class="headerlink" title="Value 数据类型的 Transformation 算子"></a>Value 数据类型的 Transformation 算子</h1><h2 id="输入分区与输出分区一对一型"><a href="#输入分区与输出分区一对一型" class="headerlink" title="输入分区与输出分区一对一型"></a>输入分区与输出分区一对一型</h2><h3 id="map-算子"><a href="#map-算子" class="headerlink" title="map 算子"></a>map 算子</h3><ul>
<li><p><strong>说明：</strong>通过将函数应用于此 RDD 的所有元素来返回新的 RDD。</p>
<p>将原来 RDD 的每个数据项通过 map 中的用户自定义函数 f 映射转变为一个新的元素。源码中 map 算子相当于初始化一个 RDD， 新 RDD 叫做 MappedRDD(this, sc.clean(f))。</p>
<p>图中每个方框表示一个 RDD 分区，左侧的分区经过用户自定义函数 <code>f:T-&gt;U</code> 映射为右侧的新 RDD 分区。但是，实际只有等到 Action 算子触发后，这个 f 函数才会和其他函数在一个 stage 中对数据进行运算。在图中的第一个分区，数据记录 V1 输入 f，通过 f 转换输出为转换后的分区中的数据记录 V&#39;1。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/map.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>](f: (<span class="type">T</span>) ⇒ <span class="type">U</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line"><span class="type">Return</span> a <span class="keyword">new</span> <span class="type">RDD</span> by applying a function to all elements of <span class="keyword">this</span> <span class="type">RDD</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.map(item =&gt; item + <span class="number">1</span>).collect</span><br><span class="line">data.map(_ + <span class="number">1</span>).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="flatMap-算子"><a href="#flatMap-算子" class="headerlink" title="flatMap 算子"></a>flatMap 算子</h3><ul>
<li><p><strong>说明：</strong>首先向该 RDD 的所有元素应用函数，然后将结果展平，以返回新的RDD。</p>
<p>将原来 RDD 中的每个元素通过函数 f 转换为新的元素，并将生成的 RDD 的每个集合中的元素合并为一个集合，内部创建 FlatMappedRDD(this，sc.clean(f))。</p>
<p>下图表示 RDD 的一个分区，进行 flatMap 函数操作，flatMap 中传入的函数为 <code>f:T-&gt;U</code>， T 和 U 可以是任意的数据类型。将分区中的数据通过用户自定义函数 f 转换为新的数据。外部大方框可以认为是一个 RDD 分区，小方框代表一个集合。 V1、 V2、 V3 在一个集合作为 RDD 的一个数据项，可能存储为数组或其他容器，转换为 V&#39;1、 V&#39;2、 V&#39;3 后，将原来的数组或容器结合拆散，拆散的数据形成为 RDD 中的数据项。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/flat_map.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>](f: (<span class="type">T</span>) ⇒ <span class="type">TraversableOnce</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line"><span class="type">Return</span> a <span class="keyword">new</span> <span class="type">RDD</span> by first applying a function to all elements of <span class="keyword">this</span> <span class="type">RDD</span>, and then flattening the results.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.flatMap(item =&gt; item to <span class="number">10</span>).collect</span><br><span class="line">data.flatMap(_ to <span class="number">10</span>).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="mapPartitions-算子"><a href="#mapPartitions-算子" class="headerlink" title="mapPartitions 算子"></a>mapPartitions 算子</h3><ul>
<li><p><strong>说明：</strong>通过将函数应用于此 RDD 的每个分区来返回新的 RDD。</p>
<p> mapPartitions 函数获取到每个分区的迭代器，在函数中通过这个分区整体的迭代器对整个分区的元素进行操作。内部实现是生成 MapPartitionsRDD。图中的方框代表一个 RDD 分区，用户通过函数 <code>f(iter)=&gt;iter.filter(_&gt;=3)</code> 对分区中所有数据进行过滤，大于和等于 3 的数据保留。一个方块代表一个 RDD 分区，含有 1、2、3 的分区过滤只剩下元素 3。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/map_partitions.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>](f: (<span class="type">Iterator</span>[<span class="type">T</span>]) ⇒ <span class="type">Iterator</span>[<span class="type">U</span>], preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>)(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line"><span class="type">Return</span> a <span class="keyword">new</span> <span class="type">RDD</span> by applying a function to each partition of <span class="keyword">this</span> <span class="type">RDD</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">func</span></span>(it: <span class="type">Iterator</span>[<span class="type">Int</span>]): <span class="type">Iterator</span>[<span class="type">Int</span>] = &#123;</span><br><span class="line">  it.filter(_ &gt;= <span class="number">3</span>)</span><br><span class="line">&#125;</span><br><span class="line">data.mapPartitions(func).collect</span><br><span class="line">data.mapPartitions(_.filter(_ &gt;= <span class="number">3</span>)).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">func: (it: <span class="type">Iterator</span>[<span class="type">Int</span>])<span class="type">Iterator</span>[<span class="type">Int</span>]</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="glom-算子"><a href="#glom-算子" class="headerlink" title="glom 算子"></a>glom 算子</h3><ul>
<li><p><strong>说明：</strong>返回通过将每个分区内的所有元素合并到数组中而创建的 RDD。</p>
<p>glom 函数将每个分区形成一个数组，内部实现是返回的 GlommedRDD。 图中的每个方框代表一个 RDD 分区。该图表示含有 V1、 V2、 V3 的分区通过函数 glom 形成一数组 Array[(V1),(V2),(V3)]。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/glom.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">glom</span></span>(): <span class="type">RDD</span>[<span class="type">Array</span>[<span class="type">T</span>]]</span><br><span class="line"><span class="type">Return</span> an <span class="type">RDD</span> created by coalescing all elements within each partition into an array.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.glom().collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Array</span>(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>), <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="type">Array</span>(<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="输入分区与输出分区多对一型"><a href="#输入分区与输出分区多对一型" class="headerlink" title="输入分区与输出分区多对一型"></a>输入分区与输出分区多对一型</h2><h3 id="union-算子"><a href="#union-算子" class="headerlink" title="union 算子"></a>union 算子</h3><ul>
<li><p><strong>说明：</strong>返回此 RDD 和另一个 RDD 的联合。</p>
<p>使用 union 函数时需要保证两个 RDD 元素的数据类型相同，返回的 RDD 数据类型和被合并的 RDD 元素数据类型相同，并不进行去重操作，保存所有元素。如果想去重可以使用 <code>distinct()</code>。同时 Spark 还提供更为简洁的使用 union 的 API，通过 ++ 符号相当于 union 函数操作。</p>
<p>图中左侧大方框代表两个 RDD，大方框内的小方框代表 RDD 的分区。右侧大方框代表合并后的 RDD，大方框内的小方框代表分区。含有 V1、V2、U1、U2、U3、U4 的 RDD 和含有 V1、V8、U5、U6、U7、U8 的 RDD 合并所有元素形成一个 RDD。V1、V1、V2、V8 形成一个分区，U1、U2、U3、U4、U5、U6、U7、U8 形成一个分区。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/union.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">union</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> the union of <span class="keyword">this</span> <span class="type">RDD</span> and another one.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>, <span class="number">3</span>)</span><br><span class="line">data1.union(data2).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>, <span class="number">15</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="cartesian-算子"><a href="#cartesian-算子" class="headerlink" title="cartesian 算子"></a>cartesian 算子</h3><ul>
<li><p><strong>说明：</strong>返回此 RDD 和另一个 RDD 的进行笛卡尔积运算，即返回 a 和 b 的所有元素对 (a,b) 的 RDD。</p>
<p>对两个 RDD 内的所有元素进行笛卡尔积操作，该操作不会执行 shuffle 操作。操作后，内部实现返回 CartesianRDD。图中左侧大方框代表两个 RDD，大方框内的小方框代表 RDD 的分区。右侧大方框代表合并后的 RDD，大方框内的小方框代表分区。图中的大方框代表 RDD，大方框中的小方框代表 RDD 分区。例如：V1 和另一个 RDD 中的 W1、W2、Q5 进行笛卡尔积运算形成 (V1,W1)、(V1,W2)、(V1,Q5)。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/cartesian.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cartesian</span></span>[<span class="type">U</span>](other: <span class="type">RDD</span>[<span class="type">U</span>])(<span class="keyword">implicit</span> arg0: <span class="type">ClassTag</span>[<span class="type">U</span>]): <span class="type">RDD</span>[(<span class="type">T</span>, <span class="type">U</span>)]</span><br><span class="line"><span class="type">Return</span> the <span class="type">Cartesian</span> product of <span class="keyword">this</span> <span class="type">RDD</span> and another one, that is, the <span class="type">RDD</span> of all pairs of elements (a, b) where a is in <span class="keyword">this</span> and b is in other.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="number">5</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data1.cartesian(data2).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">5</span>), (<span class="number">1</span>,<span class="number">6</span>), (<span class="number">1</span>,<span class="number">7</span>), (<span class="number">1</span>,<span class="number">8</span>), (<span class="number">1</span>,<span class="number">9</span>), (<span class="number">1</span>,<span class="number">10</span>), (<span class="number">2</span>,<span class="number">5</span>), (<span class="number">2</span>,<span class="number">6</span>), (<span class="number">3</span>,<span class="number">5</span>), (<span class="number">3</span>,<span class="number">6</span>), (<span class="number">2</span>,<span class="number">7</span>), (<span class="number">2</span>,<span class="number">8</span>), (<span class="number">3</span>,<span class="number">7</span>), (<span class="number">3</span>,<span class="number">8</span>), (<span class="number">2</span>,<span class="number">9</span>), (<span class="number">2</span>,<span class="number">10</span>), (<span class="number">3</span>,<span class="number">9</span>), (<span class="number">3</span>,<span class="number">10</span>), (<span class="number">4</span>,<span class="number">5</span>), (<span class="number">4</span>,<span class="number">6</span>), (<span class="number">5</span>,<span class="number">5</span>), (<span class="number">5</span>,<span class="number">6</span>), (<span class="number">4</span>,<span class="number">7</span>), (<span class="number">4</span>,<span class="number">8</span>), (<span class="number">5</span>,<span class="number">7</span>), (<span class="number">5</span>,<span class="number">8</span>), (<span class="number">4</span>,<span class="number">9</span>), (<span class="number">4</span>,<span class="number">10</span>), (<span class="number">5</span>,<span class="number">9</span>), (<span class="number">5</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="输入分区与输出分区多对多型"><a href="#输入分区与输出分区多对多型" class="headerlink" title="输入分区与输出分区多对多型"></a>输入分区与输出分区多对多型</h2><h3 id="groupBy-算子"><a href="#groupBy-算子" class="headerlink" title="groupBy 算子"></a>groupBy 算子</h3><ul>
<li><p><strong>说明：</strong>返回分组元素的 RDD。每个组由一个键和映射到该键的一系列元素组成。不能保证每个组中元素的顺序，并且每次生成的 RDD 时甚至可能会有所不同。</p>
<p>将元素通过函数生成相应的 Key，数据就转化为 Key-Value 格式，之后将 Key 相同的元素分为一组。
函数实现如下：将用户函数预处理，对数据 map 进行函数操作，最后再进行 groupByKey 分组操作。<code>this.map(t =&gt; (cleanF(t), t)).groupByKey(p)</code>，其中， p 确定了分区个数和分区函数，也就决定了并行化的程度。</p>
<p>图中方框代表一个 RDD 分区，相同 key 的元素合并到一个组。例如 V1 和 V2 合并为 V，Value 为 V1,V2。形成 V,Seq(V1,V2)。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/group_by.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>, numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupBy</span></span>[<span class="type">K</span>](f: (<span class="type">T</span>) ⇒ <span class="type">K</span>, p: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>], ord: <span class="type">Ordering</span>[<span class="type">K</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br><span class="line"><span class="type">Return</span> an <span class="type">RDD</span> of grouped elements. <span class="type">Each</span> group consists of a key and a sequence of elements mapping to that key. <span class="type">The</span> ordering of elements within each group is not guaranteed, and may even differ each time the resulting <span class="type">RDD</span> is evaluated.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">groupByKey</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br><span class="line"><span class="type">Group</span> the values <span class="keyword">for</span> each key in the <span class="type">RDD</span> into a single sequence. <span class="type">Hash</span>-partitions the resulting <span class="type">RDD</span> <span class="keyword">with</span> the existing partitioner/parallelism level. <span class="type">The</span> ordering of elements within each group is not guaranteed, and may even differ each time the resulting <span class="type">RDD</span> is evaluated.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data1.groupBy(_ % <span class="number">2</span>).collect  <span class="comment">// 分成两组</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;aa&quot;</span>, <span class="string">&quot;bb&quot;</span>, <span class="string">&quot;cc&quot;</span>, <span class="string">&quot;aaa&quot;</span>, <span class="string">&quot;bbb&quot;</span>, <span class="string">&quot;ccc&quot;</span>), <span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> data3 = data2.keyBy(_.length)  <span class="comment">// 给 value 加上 key，key 为对应 string 的长度</span></span><br><span class="line">data3.groupByKey.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((<span class="number">0</span>,<span class="type">CompactBuffer</span>(<span class="number">4</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">10</span>)), (<span class="number">1</span>,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">3</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">5</span>)))</span><br><span class="line"></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">2</span>] at keyBy at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = <span class="type">Array</span>((<span class="number">1</span>,<span class="type">CompactBuffer</span>(a, b, c)), (<span class="number">3</span>,<span class="type">CompactBuffer</span>(aaa, bbb, ccc)), (<span class="number">2</span>,<span class="type">CompactBuffer</span>(aa, bb, cc)))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="输出分区为输入分区子集型"><a href="#输出分区为输入分区子集型" class="headerlink" title="输出分区为输入分区子集型"></a>输出分区为输入分区子集型</h2><h3 id="filter-算子"><a href="#filter-算子" class="headerlink" title="filter 算子"></a>filter 算子</h3><ul>
<li><p><strong>说明：</strong>返回仅包含满足条件的元素的新 RDD。</p>
<p>filter 函数功能是对元素进行过滤，对每个元素应用 f 函 数，返回值为 true 的元素在 RDD 中保留，返回值为 false 的元素将被过滤掉。内部实现相当于生成 <code>FilteredRDD(this，sc.clean(f))</code>。图中每个方框代表一个 RDD 分区， T 可以是任意的类型。通过用户自定义的过滤函数 f，对每个数据项操作，将满足条件、返回结果为 true 的数据项保留。例如，过滤掉 V2 和 V3 保留了 V1，为区分命名为 V&#39;1。</p>
<p>下面代码为函数的本质实现：<code>def filter(f:T=&gt;Boolean):RDD[T]=newFilteredRDD(this,sc.clean(f))</code></p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/filter.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter</span></span>(f: (<span class="type">T</span>) ⇒ <span class="type">Boolean</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> a <span class="keyword">new</span> <span class="type">RDD</span> containing only the elements that satisfy a predicate.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.filter(_ % <span class="number">2</span> == <span class="number">0</span>).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="distinct-算子"><a href="#distinct-算子" class="headerlink" title="distinct 算子"></a>distinct 算子</h3><ul>
<li><p><strong>说明：</strong>返回一个包含该 RDD 中不同元素的新 RDD。</p>
<p>distinct 将 RDD 中的元素进行去重操作。图中的每个方框代表一个 RDD 分区，通过 distinct 函数，将数据去重。例如，重复数据 V1、V1 去重后只保留一份 V1。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/distinct.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distinct</span></span>(numPartitions: <span class="type">Int</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> a <span class="keyword">new</span> <span class="type">RDD</span> containing the distinct elements in <span class="keyword">this</span> <span class="type">RDD</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>), <span class="number">3</span>)</span><br><span class="line">data.distinct.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="subtract-算子"><a href="#subtract-算子" class="headerlink" title="subtract 算子"></a>subtract 算子</h3><ul>
<li><p><strong>说明：</strong>返回一个新 RDD，其中的元素在第一个 RDD 有，第二个 RDD 没有。</p>
<p>subtract 相当于进行集合的差操作，RDD1 去除 RDD1 和 RDD2 交集中的所有元素。图中左侧的大方框代表两个 RDD，大方框内的小方框代表 RDD 的分区。右侧大方框代表合并后的RDD，大方框内的小方框代表分区。V1 在两个 RDD 中均有，根据差集运算规则，新 RDD 不保留，V2 在第一个 RDD 有，第二个 RDD 没有，则在新 RDD 元素中包含 V2。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/subtract.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>]): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">subtract</span></span>(other: <span class="type">RDD</span>[<span class="type">T</span>], p: <span class="type">Partitioner</span>)(<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">T</span>] = <span class="literal">null</span>): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> an <span class="type">RDD</span> <span class="keyword">with</span> the elements from <span class="keyword">this</span> that are not in other.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">data1.subtract(data2).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">6</span>, <span class="number">9</span>, <span class="number">7</span>, <span class="number">10</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="sample-算子"><a href="#sample-算子" class="headerlink" title="sample 算子"></a>sample 算子</h3><ul>
<li><p><strong>说明：</strong>返回此 RDD 的采样子集。</p>
<p> sample 将 RDD 这个集合内的元素进行采样，获取所有元素的子集。用户可以设定是否有放回的抽样、百分比、随机种子，进而决定采样方式。内部实现是生成 SampledRDD(withReplacement, fraction, seed)。图中的每个方框是一个 RDD 分区。通过 sample 函数， 采样 50% 的数据。V1、V2、U1、U2、U3、U4 采样出数据 V1 和 U1、U2 形成新的 RDD。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/sample.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample</span></span>(withReplacement: <span class="type">Boolean</span>, fraction: <span class="type">Double</span>, seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">RDD</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> a sampled subset of <span class="keyword">this</span> <span class="type">RDD</span>.</span><br><span class="line"></span><br><span class="line">withReplacement：can elements be sampled multiple times (replaced when sampled out)</span><br><span class="line">可以多次采样元素（采样时替换），<span class="literal">true</span> 表示有放回抽样，<span class="literal">false</span> 表示无放回抽样，</span><br><span class="line"></span><br><span class="line">fraction：expected size of the sample as a fraction of <span class="keyword">this</span> <span class="type">RDD</span><span class="symbol">&#x27;s</span> size without replacement: probability that each element is chosen; fraction must be [<span class="number">0</span>, <span class="number">1</span>] <span class="keyword">with</span> replacement: expected number of times each element is chosen; fraction must be greater than or equal to <span class="number">0</span></span><br><span class="line">样本的预期大小，占该 <span class="type">RDD</span> 大小的一部分，无需替换：选择每个元素的概率；分数必须为 [<span class="number">0</span>，<span class="number">1</span>]，并带有替换：选择每个元素的预期次数；小数必须大于或等于 <span class="number">0</span></span><br><span class="line"></span><br><span class="line">seed：seed <span class="keyword">for</span> the random number generator</span><br><span class="line">随机数生成器的种子</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.sample(<span class="literal">true</span>, <span class="number">0.5</span>, <span class="number">9</span>).collect</span><br><span class="line">data.sample(<span class="literal">false</span>, <span class="number">0.5</span>, <span class="number">9</span>).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">9</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="Cache-型"><a href="#Cache-型" class="headerlink" title="Cache 型"></a>Cache 型</h2><h3 id="cache-算子"><a href="#cache-算子" class="headerlink" title="cache 算子"></a>cache 算子</h3><ul>
<li><p><strong>说明：</strong>使用默认存储级别（MEMORY_ONLY）缓存该 RDD。</p>
<p>cache 将 RDD 元素从磁盘缓存到内存。相当于 persist(MEMORY_ONLY) 函数的功能。图中每个方框代表一个 RDD 分区，左侧相当于数据分区都存储在磁盘，通过 cache 算子将数据缓存在内存。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/cache.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line"><span class="type">Persist</span> <span class="keyword">this</span> <span class="type">RDD</span> <span class="keyword">with</span> the <span class="keyword">default</span> storage level (<span class="type">MEMORY_ONLY</span>).</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.cache()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: data.<span class="keyword">type</span> = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="persist-算子"><a href="#persist-算子" class="headerlink" title="persist 算子"></a>persist 算子</h3><ul>
<li><p><strong>说明：</strong>设置此 RDD 的存储级别，以在第一次计算它之后将其值持久化到各个操作中。如果 RDD 尚未设置存储级别，则只能用于分配新的存储级别。本地检查点是一个例外。</p>
<p>persist 函数对 RDD 进行缓存操作。数据缓存在哪里依据 <code>StorageLevel</code> 这个枚举类型进行确定。有以下几种类型的组合，<code>DISK</code> 代表磁盘，<code>MEMORY</code> 代表内存，<code>SER</code> 代表数据是否进行序列化存储。</p>
<p>下面为函数定义，<code>StorageLevel</code> 是枚举类型，代表存储模式，用户可以按需进行选择。<code>persist(newLevel:StorageLevel)</code>，下面列出 persist 函数可以进行缓存的模式。例如，<code>MEMORY_AND_DISK_SER</code> 代表数据可以存储在内存和磁盘，并且以序列化的方式存储，其他同理。</p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StorageLevel</span> <span class="title">private</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useDisk: <span class="type">Boolean</span>, //是否使用磁盘</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useMemory: <span class="type">Boolean</span>, //是否使用内存</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _useOffHeap: <span class="type">Boolean</span>, //是否使用堆外内存</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _deserialized: <span class="type">Boolean</span>, //是否反序列化</span></span></span><br><span class="line"><span class="class"><span class="params">    private var _replication: <span class="type">Int</span> = 1</span>) <span class="comment">//备份因子，默认为1</span></span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Externalizable</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StorageLevel</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> <span class="type">NONE</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">DISK_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_ONLY_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">false</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">true</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">MEMORY_AND_DISK_SER_2</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="literal">false</span>, <span class="number">2</span>)</span><br><span class="line">  <span class="keyword">val</span> <span class="type">OFF_HEAP</span> = <span class="keyword">new</span> <span class="type">StorageLevel</span>(<span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">true</span>, <span class="literal">false</span>, <span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</p>
<p>下图中方框代表 RDD 分区。<code>disk</code> 代表存储在磁盘，<code>mem</code> 代表存储在内存。数据最初全部存储在磁盘，通过 <code>persist(MEMORY_AND_DISK)</code> 将数据缓存到内存，但是有的分区无法容纳在内存，将含有 V1、 V2、 V3 的 RDD 存储到磁盘，将含有 U1，U2 的 RDD 仍旧存储在内存。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/persist.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line"><span class="type">Persist</span> <span class="keyword">this</span> <span class="type">RDD</span> <span class="keyword">with</span> the <span class="keyword">default</span> storage level (<span class="type">MEMORY_ONLY</span>).</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(newLevel: <span class="type">StorageLevel</span>): <span class="type">RDD</span>.<span class="keyword">this</span>.<span class="keyword">type</span></span><br><span class="line"><span class="type">Set</span> <span class="keyword">this</span> <span class="type">RDD</span><span class="symbol">&#x27;s</span> storage level to persist its values across operations after the first time it is computed. <span class="type">This</span> can only be used to assign a <span class="keyword">new</span> storage level <span class="keyword">if</span> the <span class="type">RDD</span> does not have a storage level set yet. <span class="type">Local</span> checkpointing is an exception.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: data.<span class="keyword">type</span> = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h1 id="Key-Value-数据类型的-Transformation-算子"><a href="#Key-Value-数据类型的-Transformation-算子" class="headerlink" title="Key-Value 数据类型的 Transformation 算子"></a>Key-Value 数据类型的 Transformation 算子</h1><h2 id="输入分区与输出分区一对一"><a href="#输入分区与输出分区一对一" class="headerlink" title="输入分区与输出分区一对一"></a>输入分区与输出分区一对一</h2><h3 id="mapValues-算子"><a href="#mapValues-算子" class="headerlink" title="mapValues 算子"></a>mapValues 算子</h3><ul>
<li><p><strong>说明：</strong>在不更改键的情况下，通过映射函数传递键值对 RDD 中的每个值；这也保留了原始 RDD 的分区。</p>
<p>针对 (Key, Value) 型数据中的 Value 进行 Map 操作，而不对 Key 进行处理。图中的方框代表 RDD 分区。 <code>a=&gt;a+2</code> 代表对 (V1, 1) 这样的 Key-Value 数据对，数据只对 Value 中的 1 进行加 2 操作，返回结果为 3。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/map_values.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapValues</span></span>[<span class="type">U</span>](f: (<span class="type">V</span>) ⇒ <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br><span class="line"><span class="type">Pass</span> each value in the key-value pair <span class="type">RDD</span> through a map function without changing the keys; <span class="keyword">this</span> also retains the original <span class="type">RDD</span><span class="symbol">&#x27;s</span> partitioning.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>), <span class="number">3</span>)</span><br><span class="line">data.map(x =&gt; (x, x)).mapValues(a =&gt; (a + <span class="number">2</span>)).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="number">3</span>), (<span class="number">2</span>,<span class="number">4</span>), (<span class="number">3</span>,<span class="number">5</span>), (<span class="number">4</span>,<span class="number">6</span>), (<span class="number">5</span>,<span class="number">7</span>), (<span class="number">6</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="对单个-RDD-或两个-RDD-聚合"><a href="#对单个-RDD-或两个-RDD-聚合" class="headerlink" title="对单个 RDD 或两个 RDD 聚合"></a>对单个 RDD 或两个 RDD 聚合</h2><p><strong>单个 RDD 聚合</strong></p>
<h3 id="combineByKey-算子"><a href="#combineByKey-算子" class="headerlink" title="combineByKey 算子"></a>combineByKey 算子</h3><ul>
<li><p><strong>说明：</strong>CombineByKeyWithClassTag 的简化版本，它使用现有的分区程序/并行度级别对生成的 RDD 进行哈希分区。此方法是为了向后兼容。它不向混洗提供组合器类标签信息。</p>
<p>例如，相当于将元素为 (Int， Int) 的 RDD 转变为了 (Int, Seq[Int]) 类型元素的 RDD。图中的方框代表 RDD 分区。如图，通过 combineByKey，将 (V1,2)， (V1,1) 数据合并为 (V1, Seq(2, 1))。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/combine_by_key.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](createCombiner: (<span class="type">V</span>) ⇒ <span class="type">C</span>, mergeValue: (<span class="type">C</span>, <span class="type">V</span>) ⇒ <span class="type">C</span>, mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) ⇒ <span class="type">C</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](createCombiner: (<span class="type">V</span>) ⇒ <span class="type">C</span>, mergeValue: (<span class="type">C</span>, <span class="type">V</span>) ⇒ <span class="type">C</span>, mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) ⇒ <span class="type">C</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKey</span></span>[<span class="type">C</span>](createCombiner: (<span class="type">V</span>) ⇒ <span class="type">C</span>, mergeValue: (<span class="type">C</span>, <span class="type">V</span>) ⇒ <span class="type">C</span>, mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) ⇒ <span class="type">C</span>, partitioner: <span class="type">Partitioner</span>, mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>, serializer: <span class="type">Serializer</span> = <span class="literal">null</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)]</span><br><span class="line"></span><br><span class="line">createCombiner，createCombiner, which turns a <span class="type">V</span> into a <span class="type">C</span> (e.g., creates a one-element <span class="type">Array</span>)</span><br><span class="line">将 <span class="type">V</span> 变成 <span class="type">C</span>（例如，创建一个元素列表），<span class="type">C</span> 不存在的情况下，比如通过 <span class="type">V</span> 创建 seq <span class="type">C</span>。</span><br><span class="line"></span><br><span class="line">mergeValue, to merge a <span class="type">V</span> into a <span class="type">C</span> (e.g., adds it to the end of a <span class="type">Array</span>)</span><br><span class="line">将 <span class="type">V</span> 合并为 <span class="type">C</span>（例如，将其添加到列表的末尾），当 <span class="type">C</span> 已经存在的情况下，需要 merge，比如把 item <span class="type">V</span> 加到 seq <span class="type">C</span> 中，或者叠加。</span><br><span class="line"></span><br><span class="line">mergeCombiners, to combine two <span class="type">C</span><span class="symbol">&#x27;s</span> into a single one.</span><br><span class="line">将两个 <span class="type">C</span> 合并为一个。</span><br><span class="line"></span><br><span class="line">mapSideCombine <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">为了减小传输量，很多 combine 可以在 map 端先做，比如叠加，可以先在一个 partition 中把所有相同的 key 的 value 叠加，再 shuffle。</span><br><span class="line"></span><br><span class="line">serializerClass： <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">传输需要序列化，用户可以自定义序列化类。</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">88</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">91</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">93</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">95</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">98</span>)))</span><br><span class="line">data.combineByKey(</span><br><span class="line">    (v) =&gt; (v, <span class="number">1</span>),</span><br><span class="line">    (c: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (c._1 + v, c._2 + <span class="number">1</span>),</span><br><span class="line">    (c1: (<span class="type">Int</span>, <span class="type">Int</span>), c2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (c1._1 + c2._1, c1._2 + c2._2)</span><br><span class="line">).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((a,(<span class="number">274</span>,<span class="number">3</span>)), (b,(<span class="number">286</span>,<span class="number">3</span>)))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="reduceByKey-算子"><a href="#reduceByKey-算子" class="headerlink" title="reduceByKey 算子"></a>reduceByKey 算子</h3><ul>
<li><p><strong>说明：</strong>使用关联和可交换的归约函数合并每个键的值。在将结果发送给 reducer 之前，这还将在每个 Mapper 上本地执行合并，这与 MapReduce 中的 combiner 类似。输出将使用现有分区/并行级别进行哈希分区。</p>
<p>reduceByKey 是比 combineByKey 更简单的一种情况，只是两个值合并成一个值，(Int, Int V) to (Int, Int C)，比如叠加。所以 createCombiner reduceByKey 很简单，就是直接返回 v，而 mergeValue 和 mergeCombiners 逻辑是相同的，没有区别。图中的方框代表 RDD 分区。通过用户自定义函数 (A,B) =&gt; (A + B) 函数，将相同 key 的数据 (V1,2) 和 (V1,1) 的 value 相加运算，结果为 (V1,3)。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/reduce_by_key.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) ⇒ <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) ⇒ <span class="type">V</span>, numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) ⇒ <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Merge</span> the values <span class="keyword">for</span> each key using an associative and commutative reduce function.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)))</span><br><span class="line">data.reduceByKey((x, y) =&gt; x + y).collect</span><br><span class="line">data.reduceByKey(_ + _).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((b,<span class="number">7</span>), (a,<span class="number">3</span>))</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((b,<span class="number">7</span>), (a,<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="partitionBy-算子"><a href="#partitionBy-算子" class="headerlink" title="partitionBy 算子"></a>partitionBy 算子</h3><ul>
<li><p><strong>说明：</strong>返回使用指定分区程序分区的 RDD 的副本。</p>
<p>如果原有 RDD 的分区器和现有分区器（partitioner）一致，则不重分区，如果不一致，则相当于根据分区器生成一个新的 ShuffledRDD。图中的方框代表 RDD 分区。通过新的分区策略将原来在不同分区的 V1、 V2 数据都合并到了一个分区。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/partition_by.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">partitionBy</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Return</span> a copy of the <span class="type">RDD</span> partitioned using the specified partitioner.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;e&quot;</span>, <span class="number">5</span>), <span class="number">2</span>)</span><br><span class="line">data.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">4</span>)).glom.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = <span class="type">Array</span>(<span class="type">Array</span>((a,<span class="number">2</span>), (c,<span class="number">1</span>)), <span class="type">Array</span>((b,<span class="number">2</span>), (e,<span class="number">5</span>)), <span class="type">Array</span>(), <span class="type">Array</span>((a,<span class="number">1</span>), (b,<span class="number">3</span>)))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<p><strong>两个 RDD 聚合</strong></p>
<h3 id="cogroup-算子"><a href="#cogroup-算子" class="headerlink" title="cogroup 算子"></a>cogroup 算子</h3><ul>
<li><p><strong>说明：</strong>对在两个 RDD 中的 Key-Value 类型的元素，每个 RDD 相同 Key 的元素分别聚合为一个集合，并且返回两个 RDD 中对应 Key 的元素集合的迭代器。</p>
<p>其中，Key 和 Value，Value 是两个 RDD 下相同 Key 的两个数据集合的迭代器所构成的元组。图中的大方框代表 RDD，大方框内的小方框代表 RDD 中的分区。将 RDD1 中的数据 (U,1)、(U1,2) 和 RDD2 中的数据 (U1，2) 合并为 (U1,((1,2),(2)))。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/cogroup.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cogroup</span></span>[<span class="type">W1</span>, <span class="type">W2</span>](other1: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W1</span>)], other2: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W2</span>)], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Iterable</span>[<span class="type">V</span>], <span class="type">Iterable</span>[<span class="type">W1</span>], <span class="type">Iterable</span>[<span class="type">W2</span>]))]</span><br><span class="line"><span class="type">For</span> each key k in <span class="keyword">this</span> or other1 or other2, <span class="keyword">return</span> a resulting <span class="type">RDD</span> that contains a tuple <span class="keyword">with</span> the list of values <span class="keyword">for</span> that key in <span class="keyword">this</span>, other1 and other2.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>)))</span><br><span class="line">data1.cogroup(data2).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((a,(<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">2</span>),<span class="type">CompactBuffer</span>(<span class="number">4</span>, <span class="number">3</span>))))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h2><h3 id="join-算子"><a href="#join-算子" class="headerlink" title="join 算子"></a>join 算子</h3><ul>
<li><p><strong>说明：</strong>返回一个RDD，其中包含所有成对的元素。</p>
<p>join 对两个需要连接的 RDD 进行 cogroup 函数操作，将相同 key 的数据能够放到一个分区，在 cogroup 操作之后形成的新 RDD 对每个 key 下的元素进行笛卡尔积的操作，返回的结果再展平，对应 key 下的所有元组形成一个集合。最后返回 RDD[(K， (V， W))]。</p>
<p>下面代码为 join 的函数实现，本质是通过 cogroup 算子先进行协同划分，再通过 flatMapValues 将合并的数据打散。</p>
<p><code>this.cogroup(other, partitioner).flatMapValues&#123;case(vs, ws) =&gt; for(v&lt;-vs; w&lt;-ws) yield(v, w) &#125;</code></p>
<p>下图是对两个 RDD 的 join 操作示意图。大方框代表 RDD，小方框代表 RDD 中的分区。函数对相同 key 的元素，如 V1 为 key 做连接后结果为 (V1,(1,1)) 和 (V1,(1,2))。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/join.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">join</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">W</span>))]</span><br><span class="line"><span class="type">Return</span> an <span class="type">RDD</span> containing all pairs of elements <span class="keyword">with</span> matching keys in <span class="keyword">this</span> and other.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>)))</span><br><span class="line">data1.join(data2).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Int</span>))] = <span class="type">Array</span>((a,(<span class="number">1</span>,<span class="number">3</span>)), (a,(<span class="number">1</span>,<span class="number">4</span>)), (a,(<span class="number">2</span>,<span class="number">3</span>)), (a,(<span class="number">2</span>,<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="leftOuterJoin-和-rightOuterJoin-算子"><a href="#leftOuterJoin-和-rightOuterJoin-算子" class="headerlink" title="leftOuterJoin 和 rightOuterJoin 算子"></a>leftOuterJoin 和 rightOuterJoin 算子</h3><ul>
<li><p><strong>说明：</strong>leftOuterJoin（左外连接）和 rightOuterJoin（右外连接）相当于在 join 的基础上先判断一侧的 RDD 元素是否为空，如果为空，则填充为空。如果不为空，则将数据进行连接运算，并返回结果。</p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">leftOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">V</span>, <span class="type">Option</span>[<span class="type">W</span>]))]</span><br><span class="line"><span class="type">Perform</span> a left outer join of <span class="keyword">this</span> and other. <span class="type">For</span> each element (k, v) in <span class="keyword">this</span>, the resulting <span class="type">RDD</span> will either contain all pairs (k, (v, <span class="type">Some</span>(w))) <span class="keyword">for</span> w in other, or the pair (k, (v, <span class="type">None</span>)) <span class="keyword">if</span> no elements in other have key k. <span class="type">Hash</span>-partitions the output using the existing partitioner/parallelism level.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)]): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], numPartitions: <span class="type">Int</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rightOuterJoin</span></span>[<span class="type">W</span>](other: <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">W</span>)], partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, (<span class="type">Option</span>[<span class="type">V</span>], <span class="type">W</span>))]</span><br><span class="line"><span class="type">Perform</span> a right outer join of <span class="keyword">this</span> and other. <span class="type">For</span> each element (k, w) in other, the resulting <span class="type">RDD</span> will either contain all pairs (k, (<span class="type">Some</span>(v), w)) <span class="keyword">for</span> v in <span class="keyword">this</span>, or the pair (k, (<span class="type">None</span>, w)) <span class="keyword">if</span> no elements in <span class="keyword">this</span> have key k. <span class="type">Hash</span>-partitions the resulting <span class="type">RDD</span> using the existing partitioner/parallelism level.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data1 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> data2 = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>)))</span><br><span class="line">data1.leftOuterJoin(data2).collect</span><br><span class="line">data2.leftOuterJoin(data1).collect</span><br><span class="line">data1.rightOuterJoin(data2).collect</span><br><span class="line">data2.rightOuterJoin(data1).collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">data2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Option</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((a,(<span class="number">2</span>,<span class="type">Some</span>(<span class="number">3</span>))), (a,(<span class="number">1</span>,<span class="type">Some</span>(<span class="number">3</span>))))</span><br><span class="line">res1: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Int</span>, <span class="type">Option</span>[<span class="type">Int</span>]))] = <span class="type">Array</span>((b,(<span class="number">4</span>,<span class="type">None</span>)), (a,(<span class="number">3</span>,<span class="type">Some</span>(<span class="number">1</span>))), (a,(<span class="number">3</span>,<span class="type">Some</span>(<span class="number">2</span>))))</span><br><span class="line">res2: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">Int</span>], <span class="type">Int</span>))] = <span class="type">Array</span>((b,(<span class="type">None</span>,<span class="number">4</span>)), (a,(<span class="type">Some</span>(<span class="number">1</span>),<span class="number">3</span>)), (a,(<span class="type">Some</span>(<span class="number">2</span>),<span class="number">3</span>)))</span><br><span class="line">res3: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">Int</span>], <span class="type">Int</span>))] = <span class="type">Array</span>((a,(<span class="type">Some</span>(<span class="number">3</span>),<span class="number">2</span>)), (a,(<span class="type">Some</span>(<span class="number">3</span>),<span class="number">1</span>)))</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h1 id="Action-算子"><a href="#Action-算子" class="headerlink" title="Action 算子"></a>Action 算子</h1><p>本质上在 Action 算子中通过 SparkContext 进行了提交作业的 runJob 操作，触发了RDD DAG 的执行。</p>
<h2 id="无输出"><a href="#无输出" class="headerlink" title="无输出"></a>无输出</h2><h3 id="foreach-算子"><a href="#foreach-算子" class="headerlink" title="foreach 算子"></a>foreach 算子</h3><ul>
<li><p><strong>说明：</strong>foreach 对 RDD 中的每个元素都应用 f 函数操作，不返回 RDD 和 Array， 而是返回 Uint。下图表示 foreach 算子通过用户自定义函数对每个数据项进行操作。本例中自定义函数为 println()，控制台打印所有数据项。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/foreach.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">foreach</span></span>[<span class="type">U</span>](f: (<span class="type">T</span>) ⇒ <span class="type">U</span>)(<span class="keyword">implicit</span> executor: <span class="type">ExecutionContext</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line"><span class="keyword">var</span> sum = sc.accumulator(<span class="number">0</span>)</span><br><span class="line">data.foreach(sum += _)</span><br><span class="line">sum.value</span><br><span class="line">data.collect().foreach(println)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">sum: org.apache.spark.<span class="type">Accumulator</span>[<span class="type">Int</span>] = <span class="number">0</span></span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">15</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line"><span class="number">4</span></span><br><span class="line"><span class="number">5</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><h3 id="saveAsTextFile-算子"><a href="#saveAsTextFile-算子" class="headerlink" title="saveAsTextFile 算子"></a>saveAsTextFile 算子</h3><ul>
<li><p><strong>说明：</strong>使用元素的字符串表示形式将此 RDD 保存为文本文件。将数据输出，存储到 HDFS 的指定目录。</p>
<p>下面为 saveAsTextFile 函数的内部实现，其内部通过调用 saveAsHadoopFile 进行实现：</p>
<p><code>this.map(x =&gt; (NullWritable.get(), new Text(x.toString))).saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)</code></p>
<p>将 RDD 中的每个元素映射转变为 (null, x.toString)，然后再将其写入 HDFS。下图中左侧方框代表 RDD 分区，右侧方框代表 HDFS 的 Block。通过函数将 RDD 的每个分区存储为 HDFS 中的一个 Block。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/save_as_text_file.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"><span class="type">Save</span> <span class="keyword">this</span> <span class="type">RDD</span> as a text file, using string representations of elements.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.partitions.size</span><br><span class="line">data.saveAsTextFile(<span class="string">&quot;/test/rdd&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[scala.collection.immutable.<span class="type">Range</span>.<span class="type">Inclusive</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">[root<span class="meta">@master</span>]# hdfs dfs -ls /test/rdd</span><br><span class="line"><span class="type">Found</span> <span class="number">4</span> items</span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/_SUCCESS</span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/part<span class="number">-00000</span></span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/part<span class="number">-00001</span></span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/part<span class="number">-00002</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="saveAsObjectFile-算子"><a href="#saveAsObjectFile-算子" class="headerlink" title="saveAsObjectFile 算子"></a>saveAsObjectFile 算子</h3><ul>
<li><p><strong>说明：</strong>将此 RDD 保存为序列化对象的 SequenceFile。</p>
<p>saveAsObjectFile 将分区中的每 10 个元素组成一个 Array，然后将这个 Array 序列化，映射为 (Null, BytesWritable(Y)) 的元素，写入 HDFS 为 SequenceFile 的格式。下面代码为函数内部实现。</p>
<p><code>map(x=&gt;(NullWritable.get()，new BytesWritable(Utils.serialize(x))))</code></p>
<p>下图中的左侧方框代表 RDD 分区，右侧方框代表 HDFS 的 Block。通过函数将 RDD 的每个分区存储为 HDFS 上的一个 Block。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/save_as_object_file.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsObjectFile</span></span>(path: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"><span class="type">Save</span> <span class="keyword">this</span> <span class="type">RDD</span> as a <span class="type">SequenceFile</span> of serialized objects.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">saveAsTextFile</span></span>(path: <span class="type">String</span>, codec: <span class="type">Class</span>[_ &lt;: <span class="type">CompressionCodec</span>]): <span class="type">Unit</span></span><br><span class="line"><span class="type">Save</span> <span class="keyword">this</span> <span class="type">RDD</span> as a compressed text file, using string representations of elements.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.partitions.size</span><br><span class="line">data.saveAsObjectFile(<span class="string">&quot;/test/rdd&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[scala.collection.immutable.<span class="type">Range</span>.<span class="type">Inclusive</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">[root<span class="meta">@master</span>]# hdfs dfs -ls /test/rdd</span><br><span class="line"><span class="type">Found</span> <span class="number">4</span> items</span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/_SUCCESS</span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/part<span class="number">-00000</span></span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/part<span class="number">-00001</span></span><br><span class="line">-rw-r--r--   <span class="number">3</span> root supergroup          <span class="number">0</span> <span class="number">2021</span><span class="number">-05</span><span class="number">-07</span> <span class="number">16</span>:<span class="number">57</span> /test/rdd/part<span class="number">-00002</span></span><br><span class="line"></span><br><span class="line">[root<span class="meta">@master</span>]# hadoop fs -cat /test/rdd/part<span class="number">-00000</span></span><br><span class="line"><span class="type">SEQ</span> !org.apache.hadoop.io.<span class="type">NullWritable</span><span class="string">&quot;org.apache.hadoop.io.BytesWritableT</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h2 id="Scala-集合和数据类型"><a href="#Scala-集合和数据类型" class="headerlink" title="Scala 集合和数据类型"></a>Scala 集合和数据类型</h2><h3 id="collect-算子"><a href="#collect-算子" class="headerlink" title="collect 算子"></a>collect 算子</h3><ul>
<li><p><strong>说明：</strong>返回一个包含此 RDD 中所有元素的数组。</p>
<p>collect 相当于 toArray，toArray 已经过时不推荐使用，collect 将分布式的 RDD 返回为一个单机的 scala Array 数组。在这个数组上运用 scala 的函数式操作。下图中左侧方框代表 RDD 分区，右侧方框代表单机内存中的数组。通过函数操作，将结果返回到 Driver 程序所在的节点，以数组形式存储。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/collect.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">List</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> an array that contains all of the elements in <span class="keyword">this</span> <span class="type">RDD</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="collectAsMap-算子"><a href="#collectAsMap-算子" class="headerlink" title="collectAsMap 算子"></a>collectAsMap 算子</h3><ul>
<li><p><strong>说明：</strong>collectAsMap 对 (K，V) 型的 RDD 数据返回一个单机 HashMap。对于重复 K 的 RDD 元素，后面的元素覆盖前面的元素。</p>
<p>下图中的左侧方框代表 RDD 分区，右侧方框代表单机数组。数据通过 collectAsMap 函数返回给 Driver 程序计算结果，结果以 HashMap 形式存储。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/collect_as_map.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collectAsMap</span></span>(): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]</span><br><span class="line"><span class="type">Return</span> the key-value pairs in <span class="keyword">this</span> <span class="type">RDD</span> to the master as a <span class="type">Map</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b,2&quot;</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>)))</span><br><span class="line">data.collectAsMap</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: scala.collection.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(a -&gt; <span class="number">3</span>, b -&gt; <span class="number">2</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="reduceByKeyLocally-算子"><a href="#reduceByKeyLocally-算子" class="headerlink" title="reduceByKeyLocally 算子"></a>reduceByKeyLocally 算子</h3><ul>
<li><p><strong>说明：</strong>使用关联和可交换的 reduce 函数合并每个键的值，但是将结果作为 Map 返回。</p>
<p>实现的是先 reduce 再 collectAsMap 的功能，先对 RDD 的整体进行 reduce 操作，然后再收集所有结果返回为一个 HashMap。</p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKeyLocally</span></span>(func: <span class="type">Function2</span>[<span class="type">V</span>, <span class="type">V</span>, <span class="type">V</span>]): <span class="type">Map</span>[<span class="type">K</span>, <span class="type">V</span>]</span><br><span class="line"><span class="type">Merge</span> the values <span class="keyword">for</span> each key using an associative and commutative reduce function, but <span class="keyword">return</span> the result immediately to the master as a <span class="type">Map</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">0</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">1</span>)))</span><br><span class="line">data.reduceByKeyLocally((x, y) =&gt; x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: scala.collection.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Int</span>] = <span class="type">Map</span>(a -&gt; <span class="number">2</span>, b -&gt; <span class="number">3</span>, c -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="lookup-算子"><a href="#lookup-算子" class="headerlink" title="lookup 算子"></a>lookup 算子</h3><ul>
<li><p><strong>说明：</strong>返回 RDD 中 key 的值列表。
lookup 函数对 (Key, Value) 型的 RDD 操作，返回指定 Key 对应的元素形成的Seq。这个函数处理优化的部分在于，如果这个 RDD 包含分区器，则只会对应处理 K 所在的分区，然后返回由 (K,V) 形成的 Seq。如果 RDD 不包含分区器，则需要对全 RDD 元素进行暴力扫描处理，搜索指定 K 对应的元素。</p>
<p>下图中的左侧方框代表 RDD 分区，右侧方框代表 Seq，最后结果返回到 Driver 所在节点的应用中。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/lookup.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lookup</span></span>(key: <span class="type">K</span>): <span class="type">List</span>[<span class="type">V</span>]</span><br><span class="line"><span class="type">Return</span> the list of values in the <span class="type">RDD</span> <span class="keyword">for</span> key.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">0</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>)))</span><br><span class="line">data.lookup(<span class="string">&quot;a&quot;</span>)</span><br><span class="line">data.lookup(<span class="string">&quot;b&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">WrappedArray</span>(<span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">res1: <span class="type">Seq</span>[<span class="type">Int</span>] = <span class="type">WrappedArray</span>(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="count-算子"><a href="#count-算子" class="headerlink" title="count 算子"></a>count 算子</h3><ul>
<li><p><strong>说明：</strong> 返回整个 RDD 的元素个数。</p>
<p>下图中，返回数据的个数为 5。一个方块代表一个 RDD 分区。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/count.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count</span></span>(): <span class="type">Long</span></span><br><span class="line"><span class="type">Return</span> the number of elements in the <span class="type">RDD</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">data.count</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Long</span> = <span class="number">5</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="top-算子"><a href="#top-算子" class="headerlink" title="top 算子"></a>top 算子</h3><ul>
<li><p><strong>说明：</strong>返回此 RDD 中的前 k 个（最大）元素，并保持顺序。</p>
<p>相近函数:</p>
<ul>
<li>top 返回最大的 k 个元素。</li>
<li>take 返回最小的 k 个元素。</li>
<li>takeOrdered 返回前 k 个（最小）元素，并且在返回的数组中保持元素的顺序。</li>
<li>first 相当于 top(1) 返回整个 RDD 中的前 k 个元素。</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>): <span class="type">List</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Returns</span> the top k (largest) elements from <span class="keyword">this</span> <span class="type">RDD</span> using the natural ordering <span class="keyword">for</span> <span class="type">T</span> and maintains the order.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top</span></span>(num: <span class="type">Int</span>, comp: <span class="type">Comparator</span>[(<span class="type">K</span>, <span class="type">V</span>)]): <span class="type">List</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Returns</span> the top k (largest) elements from <span class="keyword">this</span> <span class="type">RDD</span> as defined by the specified <span class="type">Comparator</span>[<span class="type">T</span>] and maintains the order.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span></span>(num: <span class="type">Int</span>): <span class="type">List</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Take</span> the first num elements of the <span class="type">RDD</span>.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>): <span class="type">List</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Returns</span> the first k (smallest) elements from <span class="keyword">this</span> <span class="type">RDD</span> using the natural ordering <span class="keyword">for</span> <span class="type">T</span> <span class="keyword">while</span> maintain the order.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeOrdered</span></span>(num: <span class="type">Int</span>, comp: <span class="type">Comparator</span>[(<span class="type">K</span>, <span class="type">V</span>)]): <span class="type">List</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br><span class="line"><span class="type">Returns</span> the first k (smallest) elements from <span class="keyword">this</span> <span class="type">RDD</span> as defined by the specified <span class="type">Comparator</span>[<span class="type">T</span>] and maintains the order.</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">first</span></span>(): (<span class="type">K</span>, <span class="type">V</span>)</span><br><span class="line"><span class="type">Return</span> the first element in <span class="keyword">this</span> <span class="type">RDD</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">data.top(<span class="number">3</span>)</span><br><span class="line">data.take(<span class="number">3</span>)</span><br><span class="line">data.takeOrdered(<span class="number">3</span>)</span><br><span class="line">data.first</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">10</span>, <span class="number">9</span>, <span class="number">8</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">res3: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="reduce-算子"><a href="#reduce-算子" class="headerlink" title="reduce 算子"></a>reduce 算子</h3><ul>
<li><p><strong>说明：</strong>使用指定的可交换和关联的二进制运算符对 RDD 的元素进行运算。</p>
<p>reduce 函数相当于对 RDD 中的元素进行 reduceLeft 函数的操作。</p>
<p>reduceLeft 先对两个元素 (K, V) 进行 reduce 函数操作，然后将结果和迭代器取出的下一个元素 (K, V) 进行 reduce 函数操作，直到迭代器遍历完所有元素，得到最后结果。在RDD中，先对每个分区中的所有元素 (K, V) 的集合分别进行 reduceLeft。每个分区形成的结果相当于一个元素 (K, V)，再对这个结果集合进行 reduceLeft 操作。</p>
<p>下图中的方框代表一个 RDD 分区，通过用户自定函数 f 将数据进行 reduce 运算。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/reduce.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(f: <span class="type">Function2</span>[(<span class="type">K</span>, <span class="type">V</span>), (<span class="type">K</span>, <span class="type">V</span>), (<span class="type">K</span>, <span class="type">V</span>)]): (<span class="type">K</span>, <span class="type">V</span>)</span><br><span class="line"><span class="type">Reduces</span> the elements of <span class="keyword">this</span> <span class="type">RDD</span> using the specified commutative and associative binary operator.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">2</span>)))</span><br><span class="line">data.reduce((x, y) =&gt; (x._1 + <span class="string">&quot;@&quot;</span> + y._1, x._2 + y._2))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: (<span class="type">String</span>, <span class="type">Int</span>) = (c<span class="meta">@d</span><span class="meta">@a</span><span class="meta">@b</span>,<span class="number">6</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="fold-算子"><a href="#fold-算子" class="headerlink" title="fold 算子"></a>fold 算子</h3><ul>
<li><p><strong>说明：</strong>使用给定的关联函数和中性的“零值”，汇总每个分区的元素，然后汇总所有分区的结果。</p>
<p>fold 和 reduce的原理相同，但是与 reduce 不同，相当于每个 reduce 时，迭代器取的第一个元素是 zeroValue。</p>
<p>下图中通过下面的用户自定义函数进行 fold 运算，图中的一个方框代表一个 RDD 分区。读者可以参照 reduce 函数理解。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/fold.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fold</span></span>(zeroValue: (<span class="type">K</span>, <span class="type">V</span>))(f: <span class="type">Function2</span>[(<span class="type">K</span>, <span class="type">V</span>), (<span class="type">K</span>, <span class="type">V</span>), (<span class="type">K</span>, <span class="type">V</span>)]): (<span class="type">K</span>, <span class="type">V</span>)</span><br><span class="line"><span class="type">Aggregate</span> the elements of each partition, and then the results <span class="keyword">for</span> all the partitions, using a given associative function and a neutral <span class="string">&quot;zero value&quot;</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)), <span class="number">2</span>)</span><br><span class="line">data.fold((<span class="string">&quot;A&quot;</span>, <span class="number">10</span>))((x, y) =&gt; (x._1 + <span class="string">&quot;@&quot;</span> + y._1, x._2 + y._2))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="type">A</span><span class="meta">@A</span><span class="meta">@a</span><span class="meta">@b</span><span class="meta">@A</span><span class="meta">@c</span><span class="meta">@d</span>,<span class="number">36</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="aggregate-算子"><a href="#aggregate-算子" class="headerlink" title="aggregate 算子"></a>aggregate 算子</h3><ul>
<li><p><strong>说明：</strong>使用给定的合并功能和中性的“零值”，汇总每个分区的元素，然后汇总所有分区的结果。此函数可以返回与该 RDD 的类型 T 不同的结果类型U。因此，我们需要一个将 T 合并为 U 的操作，以及一个将两个 U 合并的操作。</p>
<p>aggregate 先对每个分区的所有元素进行 aggregate 操作，再对分区的结果进行 fold 操作。</p>
<p>aggregate 与 fold 和 reduce 的不同之处在于，aggregate 相当于采用归并的方式进行数据聚合，这种聚合是并行化的。而在 fold 和 reduce 函数的运算过程中，每个分区中需要进行串行处理，每个分区串行计算完结果，结果再按之前的方式进行聚合，并返回最终聚合结果。</p>
<p>下图图通过用户自定义函数对 RDD 进行 aggregate 的聚合操作，图中的每个方框代表一个 RDD 分区。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/aggregate.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aggregate</span></span>[<span class="type">U</span>](zeroValue: <span class="type">U</span>)(seqOp: <span class="type">Function2</span>[<span class="type">U</span>, (<span class="type">K</span>, <span class="type">V</span>), <span class="type">U</span>], combOp: <span class="type">Function2</span>[<span class="type">U</span>, <span class="type">U</span>, <span class="type">U</span>]): <span class="type">U</span></span><br><span class="line"><span class="type">Aggregate</span> the elements of each partition, and then the results <span class="keyword">for</span> all the partitions, using given combine functions and a neutral <span class="string">&quot;zero value&quot;</span>.</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)), <span class="number">2</span>)</span><br><span class="line">data.aggregate((<span class="string">&quot;A&quot;</span>, <span class="number">10</span>))((x, y) =&gt; (x._1 + <span class="string">&quot;@&quot;</span> + y._1, x._2 + y._2), (x, y) =&gt; (x._1 + <span class="string">&quot;@&quot;</span> + y._1, x._2 + y._2))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: (<span class="type">String</span>, <span class="type">Int</span>) = (<span class="type">A</span><span class="meta">@A</span><span class="meta">@a</span><span class="meta">@b</span><span class="meta">@A</span><span class="meta">@c</span><span class="meta">@d</span>,<span class="number">36</span>)</span><br></pre></td></tr></table></figure>
</p>
</li>
</ul>
<h3 id="takeSample-算子"><a href="#takeSample-算子" class="headerlink" title="takeSample 算子"></a>takeSample 算子</h3><ul>
<li><p><strong>说明：</strong>在数组中返回此 RDD 的固定大小的采样子集</p>
<p>takeSample 函数和上面的 sample 函数是一个原理，但是不使用相对比例采样，而是按设定的采样个数进行采样，同时返回结果不再是 RDD，而是相当于对采样后的数据进行 collect，返回结果的集合为单机的数组。图中左侧的方框代表分布式的各个节点上的分区，右侧方框代表单机上返回的结果数组。通过 takeSample 对数据采样，设置为采样一份数据，返回结果为 V1。</p>
<p><img src= "https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif" data-lazy-src="/images/post/spark_rdd/take_sample.png" alt></p>
</li>
<li><p><strong>用法：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">takeSample</span></span>(withReplacement: <span class="type">Boolean</span>, num: <span class="type">Int</span>, seed: <span class="type">Long</span> = <span class="type">Utils</span>.random.nextLong): <span class="type">Array</span>[<span class="type">T</span>]</span><br><span class="line"><span class="type">Return</span> a fixed-size sampled subset of <span class="keyword">this</span> <span class="type">RDD</span> in an array</span><br><span class="line"></span><br><span class="line">withReplacement</span><br><span class="line">whether sampling is done <span class="keyword">with</span> replacement</span><br><span class="line">是否通过更换进行采样</span><br><span class="line"></span><br><span class="line">num</span><br><span class="line">size of the returned sample</span><br><span class="line">返回样本的大小</span><br><span class="line"></span><br><span class="line">seed</span><br><span class="line">seed <span class="keyword">for</span> the random number generator</span><br><span class="line">随机数生成器的种子</span><br></pre></td></tr></table></figure></p>
</li>
<li><p><strong>示例：</strong></p>
<p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>, <span class="number">3</span>)</span><br><span class="line">data.takeSample(<span class="literal">true</span>, <span class="number">1</span>, <span class="number">9</span>)</span><br><span class="line">data.takeSample(<span class="literal">false</span>, <span class="number">1</span>, <span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 输出结果</span></span><br><span class="line">data: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at parallelize at &lt;console&gt;:<span class="number">25</span></span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>)</span><br><span class="line">res1: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>)</span><br></pre></td></tr></table></figure></p>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Spark RDD 常用算子</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a id="post-url" href="https://blog.eurkon.com/post/c55e5115.html">https://blog.eurkon.com/post/c55e5115.html</a><a id="post-url-copy"><i class="fas fa-paste copy-button"></i></a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>Eurkon</h></div></div><div class="post-copyright-p"><h>发布于</h><div class="post-copyright-cc-info"><h>2021-05-07</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2021-05-20</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" title="Creative Commons" target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a title="CC BY-NC-SA 4.0" target="_blank" rel="noopener external nofollow noreferrer" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><div class="post_share"><div class="social-share" data-image="/images/cover/spark_rdd.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button button--animated"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/wechat.jpg" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/alipay.jpg" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/post/61763977.html"><img class="prev-cover" src="/images/cover/hexo.svg" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Hexo 博客实时访问统计图</div></div></a></div><div class="next-post pull-right"><a href="/post/817c7d82.html"><img class="next-cover" src="/images/cover/linux_command.png" onerror="onerror=null;src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Linux 常用命令</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/post/7e24cf66.html" title="大数据面试题解析"><img class="cover" src="/images/cover/bigdata_interview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-25</div><div class="title">大数据面试题解析</div></div></a></div><div><a href="/post/cccf27af.html" title="Spark 面试题解析"><img class="cover" src="/images/cover/bigdata_interview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-26</div><div class="title">Spark 面试题解析</div></div></a></div><div><a href="/post/5a2a12a6.html" title="Flume 面试题解析"><img class="cover" src="/images/cover/bigdata_interview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-29</div><div class="title">Flume 面试题解析</div></div></a></div><div><a href="/post/d1e7dfd3.html" title="HDFS Shell 命令"><img class="cover" src="/images/cover/hdfs_shell.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-05</div><div class="title">HDFS Shell 命令</div></div></a></div><div><a href="/post/420614eb.html" title="Sqoop 面试题解析"><img class="cover" src="/images/cover/bigdata_interview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-13</div><div class="title">Sqoop 面试题解析</div></div></a></div><div><a href="/post/2f1ea7f2.html" title="Zookeeper 面试题解析"><img class="cover" src="/images/cover/bigdata_interview.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-27</div><div class="title">Zookeeper 面试题解析</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-info-avatar is-center"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/avatar.jpg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/loading.gif'" alt="avatar"/><div class="author-info__name">Eurkon</div><div class="author-info__description">不只是代码的搬运工</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">47</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">24</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">3</div></a></div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Eurkon" rel="external nofollow noreferrer" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:eurkon@foxmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-%E7%9A%84%E7%AE%97%E5%AD%90%E7%9A%84%E5%88%86%E7%B1%BB"><span class="toc-number">1.</span> <span class="toc-text">Spark 的算子的分类</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Value-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84-Transformation-%E7%AE%97%E5%AD%90"><span class="toc-number">2.</span> <span class="toc-text">Value 数据类型的 Transformation 算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%88%86%E5%8C%BA%E4%B8%8E%E8%BE%93%E5%87%BA%E5%88%86%E5%8C%BA%E4%B8%80%E5%AF%B9%E4%B8%80%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">输入分区与输出分区一对一型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#map-%E7%AE%97%E5%AD%90"><span class="toc-number">2.1.1.</span> <span class="toc-text">map 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flatMap-%E7%AE%97%E5%AD%90"><span class="toc-number">2.1.2.</span> <span class="toc-text">flatMap 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapPartitions-%E7%AE%97%E5%AD%90"><span class="toc-number">2.1.3.</span> <span class="toc-text">mapPartitions 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glom-%E7%AE%97%E5%AD%90"><span class="toc-number">2.1.4.</span> <span class="toc-text">glom 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%88%86%E5%8C%BA%E4%B8%8E%E8%BE%93%E5%87%BA%E5%88%86%E5%8C%BA%E5%A4%9A%E5%AF%B9%E4%B8%80%E5%9E%8B"><span class="toc-number">2.2.</span> <span class="toc-text">输入分区与输出分区多对一型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#union-%E7%AE%97%E5%AD%90"><span class="toc-number">2.2.1.</span> <span class="toc-text">union 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cartesian-%E7%AE%97%E5%AD%90"><span class="toc-number">2.2.2.</span> <span class="toc-text">cartesian 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%88%86%E5%8C%BA%E4%B8%8E%E8%BE%93%E5%87%BA%E5%88%86%E5%8C%BA%E5%A4%9A%E5%AF%B9%E5%A4%9A%E5%9E%8B"><span class="toc-number">2.3.</span> <span class="toc-text">输入分区与输出分区多对多型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#groupBy-%E7%AE%97%E5%AD%90"><span class="toc-number">2.3.1.</span> <span class="toc-text">groupBy 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%87%BA%E5%88%86%E5%8C%BA%E4%B8%BA%E8%BE%93%E5%85%A5%E5%88%86%E5%8C%BA%E5%AD%90%E9%9B%86%E5%9E%8B"><span class="toc-number">2.4.</span> <span class="toc-text">输出分区为输入分区子集型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#filter-%E7%AE%97%E5%AD%90"><span class="toc-number">2.4.1.</span> <span class="toc-text">filter 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distinct-%E7%AE%97%E5%AD%90"><span class="toc-number">2.4.2.</span> <span class="toc-text">distinct 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#subtract-%E7%AE%97%E5%AD%90"><span class="toc-number">2.4.3.</span> <span class="toc-text">subtract 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sample-%E7%AE%97%E5%AD%90"><span class="toc-number">2.4.4.</span> <span class="toc-text">sample 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cache-%E5%9E%8B"><span class="toc-number">2.5.</span> <span class="toc-text">Cache 型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cache-%E7%AE%97%E5%AD%90"><span class="toc-number">2.5.1.</span> <span class="toc-text">cache 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#persist-%E7%AE%97%E5%AD%90"><span class="toc-number">2.5.2.</span> <span class="toc-text">persist 算子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Key-Value-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84-Transformation-%E7%AE%97%E5%AD%90"><span class="toc-number">3.</span> <span class="toc-text">Key-Value 数据类型的 Transformation 算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%88%86%E5%8C%BA%E4%B8%8E%E8%BE%93%E5%87%BA%E5%88%86%E5%8C%BA%E4%B8%80%E5%AF%B9%E4%B8%80"><span class="toc-number">3.1.</span> <span class="toc-text">输入分区与输出分区一对一</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#mapValues-%E7%AE%97%E5%AD%90"><span class="toc-number">3.1.1.</span> <span class="toc-text">mapValues 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AF%B9%E5%8D%95%E4%B8%AA-RDD-%E6%88%96%E4%B8%A4%E4%B8%AA-RDD-%E8%81%9A%E5%90%88"><span class="toc-number">3.2.</span> <span class="toc-text">对单个 RDD 或两个 RDD 聚合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#combineByKey-%E7%AE%97%E5%AD%90"><span class="toc-number">3.2.1.</span> <span class="toc-text">combineByKey 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKey-%E7%AE%97%E5%AD%90"><span class="toc-number">3.2.2.</span> <span class="toc-text">reduceByKey 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#partitionBy-%E7%AE%97%E5%AD%90"><span class="toc-number">3.2.3.</span> <span class="toc-text">partitionBy 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cogroup-%E7%AE%97%E5%AD%90"><span class="toc-number">3.2.4.</span> <span class="toc-text">cogroup 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%9E%E6%8E%A5"><span class="toc-number">3.3.</span> <span class="toc-text">连接</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#join-%E7%AE%97%E5%AD%90"><span class="toc-number">3.3.1.</span> <span class="toc-text">join 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#leftOuterJoin-%E5%92%8C-rightOuterJoin-%E7%AE%97%E5%AD%90"><span class="toc-number">3.3.2.</span> <span class="toc-text">leftOuterJoin 和 rightOuterJoin 算子</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Action-%E7%AE%97%E5%AD%90"><span class="toc-number">4.</span> <span class="toc-text">Action 算子</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A0%E8%BE%93%E5%87%BA"><span class="toc-number">4.1.</span> <span class="toc-text">无输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#foreach-%E7%AE%97%E5%AD%90"><span class="toc-number">4.1.1.</span> <span class="toc-text">foreach 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#HDFS"><span class="toc-number">4.2.</span> <span class="toc-text">HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsTextFile-%E7%AE%97%E5%AD%90"><span class="toc-number">4.2.1.</span> <span class="toc-text">saveAsTextFile 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsObjectFile-%E7%AE%97%E5%AD%90"><span class="toc-number">4.2.2.</span> <span class="toc-text">saveAsObjectFile 算子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scala-%E9%9B%86%E5%90%88%E5%92%8C%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B"><span class="toc-number">4.3.</span> <span class="toc-text">Scala 集合和数据类型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#collect-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.1.</span> <span class="toc-text">collect 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collectAsMap-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.2.</span> <span class="toc-text">collectAsMap 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKeyLocally-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.3.</span> <span class="toc-text">reduceByKeyLocally 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lookup-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.4.</span> <span class="toc-text">lookup 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.5.</span> <span class="toc-text">count 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#top-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.6.</span> <span class="toc-text">top 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduce-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.7.</span> <span class="toc-text">reduce 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fold-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.8.</span> <span class="toc-text">fold 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aggregate-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.9.</span> <span class="toc-text">aggregate 算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#takeSample-%E7%AE%97%E5%AD%90"><span class="toc-number">4.3.10.</span> <span class="toc-text">takeSample 算子</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-clock"><div class="card-content"><div id="electric_clock"><img id="card-clock-loading" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/clock_loading.gif" data-ll-status="loading" style="height:108px; width:100%;"/></div></div></div><script defer="defer" data-pjax="data-pjax" src="https://pv.sohu.com/cityjson?ie=utf-8"></script><script defer="defer" data-pjax="data-pjax" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/js/card_clock.js"></script><div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px; overflow: hidden;"><div class="history_swiper-container" id="history-container" style="width: 100%; height: 100%;"><div class="swiper-wrapper" id="history_container_wrapper" style="height: 20px;"></div></div></div></div></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.css" media="print" onload="this.media='all'"/><script defer="defer" data-pjax="data-pjax" src="https://cdn.jsdelivr.net/npm/swiper/swiper-bundle.min.js"></script><script defer="defer" data-pjax="data-pjax" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/js/card_history.js"></script><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/post/cccf27af.html" title="Spark 面试题解析"><img src="/images/cover/bigdata_interview.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="Spark 面试题解析"/></a><div class="content"><a class="title" href="/post/cccf27af.html" title="Spark 面试题解析">Spark 面试题解析</a><time datetime="2021-05-26T01:00:00.000Z" title="发表于 2021-05-26 09:00:00">2021-05-26</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/1a169b5a.html" title="Hexo 博客访问日历图"><img src="/images/cover/hexo.svg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="Hexo 博客访问日历图"/></a><div class="content"><a class="title" href="/post/1a169b5a.html" title="Hexo 博客访问日历图">Hexo 博客访问日历图</a><time datetime="2021-05-17T04:00:00.000Z" title="发表于 2021-05-17 12:00:00">2021-05-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/61763977.html" title="Hexo 博客实时访问统计图"><img src="/images/cover/hexo.svg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="Hexo 博客实时访问统计图"/></a><div class="content"><a class="title" href="/post/61763977.html" title="Hexo 博客实时访问统计图">Hexo 博客实时访问统计图</a><time datetime="2021-05-17T01:00:00.000Z" title="发表于 2021-05-17 09:00:00">2021-05-17</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/c55e5115.html" title="Spark RDD 常用算子"><img src="/images/cover/spark_rdd.jpg" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="Spark RDD 常用算子"/></a><div class="content"><a class="title" href="/post/c55e5115.html" title="Spark RDD 常用算子">Spark RDD 常用算子</a><time datetime="2021-05-07T01:00:00.000Z" title="发表于 2021-05-07 09:00:00">2021-05-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/817c7d82.html" title="Linux 常用命令"><img src="/images/cover/linux_command.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="Linux 常用命令"/></a><div class="content"><a class="title" href="/post/817c7d82.html" title="Linux 常用命令">Linux 常用命令</a><time datetime="2021-04-19T01:00:00.000Z" title="发表于 2021-04-19 09:00:00">2021-04-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/post/54dbb40b.html" title="MySQL 面试题解析"><img src="/images/cover/bigdata_interview.png" onerror="this.onerror=null;this.src='https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/images/user/404.jpg'" alt="MySQL 面试题解析"/></a><div class="content"><a class="title" href="/post/54dbb40b.html" title="MySQL 面试题解析">MySQL 面试题解析</a><time datetime="2021-04-14T01:00:00.000Z" title="发表于 2021-04-14 09:00:00">2021-04-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 By Eurkon</div><div class="footer_custom_text"><p id="badges"><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer" style="margin-inline: 5px;" one-link-mark="yes"><img title="博客框架Hexo" class="entered loaded" src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" data-ll-status="loaded"></a><a target="_blank" href="https://butterfly.js.org/" rel="external nofollow noreferrer" style="margin-inline: 5px;" one-link-mark="yes"><img title="博客主题Butterfly" class="entered loaded" src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" data-ll-status="loaded"></a><a target="_blank" href="https://www.jsdelivr.com/" rel="external nofollow noreferrer" style="margin-inline: 5px;" one-link-mark="yes"><img title="本站使用JsDelivr为静态资源提供CDN加速" class="entered loaded" src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr" data-ll-status="loaded"></a><a target="_blank" href="https://vercel.com/" rel="external nofollow noreferrer" style="margin-inline: 5px;" one-link-mark="yes"><img title="本站部署托管于Vercel" class="entered loading" src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" data-ll-status="loaded" ></a><a target="_blank" href="https://github.com/" rel="external nofollow noreferrer" style="margin-inline: 5px;" one-link-mark="yes"><img title="本站项目由GitHub托管" class="entered loaded" src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" data-ll-status="loaded"></a><a target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" style="margin-inline: 5px;" one-link-mark="yes"><img title="本站采用创作共用-署名-非商业性-相同方式共享4.0版国际许可协议" class="entered loaded" src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" data-ll-status="loaded"></a></p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font-plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font-minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><script src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/js/search/local-search.js"></script><div class="js-pjax"><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(() => {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'dark',
      })
      true && mermaid.init()
    })
  }
}</script><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'twikoo-7gsa2rc0c087a562',
      region: ''
    }, null))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'twikoo-7gsa2rc0c087a562',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'twikoo-7gsa2rc0c087a562',
        region: '',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer data-pjax src="https://cdn.jsdelivr.net/gh/briangonzalez/rgbaster.js@1.1.0/rgbaster.min.js"></script><script defer data-pjax src="/js/custom.js"></script><script defer data-pjax async src="//at.alicdn.com/t/font_2358265_expoyqe85d4.js"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener toc scroll 
  window.removeEventListener('scroll', window.tocScrollFn)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><div class="pjax-reload"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer="defer" src="https://cdn.jsdelivr.net/gh/graingert/wow@1.3.0/dist/wow.min.js"></script><script defer="defer" src="https://cdn.jsdelivr.net/gh/Eurkon/CDN/hexo/js/wow_init.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start -->
    <script data-pjax>
    if (document.getElementById('catalog_magnet')) {
      document.getElementById('catalog_magnet').innerHTML = '<div class="magnet_item"><a class="magnet_link" href="/categories/生活随记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💬 Eurkonの生活随记 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="/categories/教程笔记/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📖 Eurkonの教程笔记 (45)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="/categories/作品案例/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🖥️ Eurkonの作品案例 (1)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a>'
    }
    </script>
    </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: var(--global-bg);margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: var(--btn-bg)}.magnet_link_more{color:var(--font-color)}.magnet_link{color:var(--font-color)}.magnet_link:hover{color: #fff}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
      <script data-pjax>
        if(document.getElementById('swiper_container')){
          document.getElementById('swiper_container').innerHTML = '<div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="/images/cover/bigdata_list.jpg" alt="/images/cover/bigdata_list.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-01-07</span><a class="blog-slider__title" href="post/fc3d946f.html">大数据精选清单</a><div class="blog-slider__text">有关大数据资料的精选清单</div><a class="blog-slider__button" href="post/fc3d946f.html">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="/images/cover/bigdata_interview.png" alt="/images/cover/bigdata_interview.png"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-03-25</span><a class="blog-slider__title" href="post/7e24cf66.html">大数据面试题解析</a><div class="blog-slider__text">大数据面试题解析</div><a class="blog-slider__button" href="post/7e24cf66.html">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="/images/cover/java_interview.jpg" alt="/images/cover/java_interview.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-03-15</span><a class="blog-slider__title" href="post/d2aef718.html">Java 面试题解析</a><div class="blog-slider__text">Java 面试题解析</div><a class="blog-slider__button" href="post/d2aef718.html">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="/images/cover/markdown_grammar.jpg" alt="/images/cover/markdown_grammar.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-01-03</span><a class="blog-slider__title" href="post/c894e39a.html">Markdown 基本语法</a><div class="blog-slider__text">Markdown 基本语法</div><a class="blog-slider__button" href="post/c894e39a.html">详情</a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div>'
        }
        </script>
      <script data-pjax src="https://cdnjs.cloudflare.com/ajax/libs/Swiper/4.1.6/js/swiper.min.js"></script>
      <script data-pjax>
        var swiper = new Swiper('.blog-slider', {
          passiveListeners: true,
          spaceBetween: 30,
          effect: 'fade',
          loop: true,
          autoplay: {
            disableOnInteraction: true,
            delay: 3000
          },
          observer: true,//修改swiper自己或子元素时，自动初始化swiper
          observeParents: true,//修改swiper的父元素时，自动初始化swiper
          mousewheel: false,
          // autoHeight: true,
          pagination: {
            el: '.blog-slider__pagination',
            clickable: true,
          }
        });
        
        if (document.getElementById('swiper_container')) {
          var comtainer = document.getElementById('swiper_container');
          comtainer.onmouseenter = function () {
            swiper.autoplay.stop();
          };
          comtainer.onmouseleave = function () {
            swiper.autoplay.start();
          }
        }
      </script><!-- hexo injector body_end end --><script async>window.onload=function(){var a=document.createElement('script'),b=document.getElementsByTagName('script')[0];a.type='text/javascript',a.async=!0,a.src='/sw-register.js?v='+Date.now(),b.parentNode.insertBefore(a,b)};</script></body></html>